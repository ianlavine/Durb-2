# ğŸ§  Durba Reinforcement Learning Agent

A **Graph Neural Network (GNN)** + **Maskable PPO** agent trained to play the **Durba** game â€” a node-based strategy environment where players capture and defend nodes by transferring "juice" through a dynamic directed graph.

This project integrates **Stable Baselines3**, **SB3-Contrib (Maskable PPO)**, and **PyTorch Geometric** to train reinforcement learning agents that can reason over structured graph environments.

---

## ğŸš€ Features

- ğŸ§© **Custom Graph Environment (`DurbaEnv`)** â€“ Node/edge-based resource control system  
- ğŸ§  **Graph Neural Network Policy** â€“ Implemented using a custom `GraphFeatureExtractor`  
- ğŸ¯ **Maskable PPO** â€“ Learns valid actions dynamically using action masks  
- ğŸ“Š **TensorBoard Logging & Checkpointing** â€“ Automatic saving every 50K steps  
- ğŸ¥ **Graph Visualization** â€“ Generates live and recorded game videos with annotations  

---

## ğŸ“ Project Structure

```
durba-rl/
â”‚
â”œâ”€â”€ train_maskppo.py # Train Maskable PPO agent
â”œâ”€â”€ evaluate_agent.py # Evaluate trained model performance
â”œâ”€â”€ play_game.py # Visualize and record agent gameplay
â”‚
â”œâ”€â”€ feature_extractors.py # Custom GNN feature extractor for policy
â”‚
â”œâ”€â”€ durba/
â”‚ â””â”€â”€ envs/
â”‚ â”œâ”€â”€ durba_env.py # Core environment logic
â”‚ â”œâ”€â”€ custom_monitor.py # Metric tracking and callbacks
â”‚ â”œâ”€â”€ agent.py # Contain agent actions execution script
â”‚ â”œâ”€â”€ rew_shaper.py # Reward Shaping
â”‚
â”œâ”€â”€ ppomask_checkpoints/ # Model checkpoints
â”œâ”€â”€ outputs/ # Generated videos and logs
â”œâ”€â”€ logs/ # TensorBoard logs
â”‚
â”œâ”€â”€ requirements.txt # Dependency list
â””â”€â”€ README.md # Project documentation

```
---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/ianlavine/Durb-2.git
cd Durba-2/backened/durba-rl
```

---

### 2ï¸âƒ£ Create a Python environment
```bash
conda create -n durba_rl python=3.12
conda activate durba_rl
```

---

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

ğŸ’¡ PyTorch with CUDA support is already included in requirements.txt.

---

## ğŸ§  Training the Agent

Train the Maskable PPO agent using your custom attention based graph feature extractor:

```bash
python train_maskppo.py
```
- The environment automatically provides valid action masks.
- Model checkpoints are saved every 50,000 steps in ```ppomask_checkpoints/.```
- TensorBoard logs are saved to ``` logs/durba_maskppo_single_discrete/.```

### ğŸ“ˆ View Training Progress

```bash
tensorboard --logdir logs/
```
![Alt text](./training_plots/Screenshot%20from%202025-10-14%2016-57-49.png "Performance Metrices")
![Alt text](./training_plots/rollout_ep_rew_mean.svg "Rewards over Steps")

---

## ğŸ§ª Evaluating the Agent
Run the following script to evaluate the trained model on multiple games:
```bash
python evaluate_agent.py
```

Example output:
```bash
No. of Games: 500
Win Rate: 0.84
Draw Rate: 0.05
```

You can adjust the number of games or model path inside evaluate_agent.py.

## ğŸ® Visualizing Gameplay
To record a full gameplay video with live graph updates:
```bash
python play_game.py
```
This script:

- Renders the graph using Matplotlib

- Displays nodes, edges, and ownership in color

- Saves an .mp4 file and step-by-step log

### ğŸ“‚ Output files:

```bash
outputs/graph_video_<timestamp>.mp4
out_logs.txt
```

## ğŸ§‘â€ğŸ”¬ Future Improvements
- [ ] Add YAML config support for hyperparameters
- [ ] Add diverse bot with different personalities to make agent learning better  
- [ ] Add self-play to further improve the performance of the agent 
- [ ] Add difficulty levels for agents  

